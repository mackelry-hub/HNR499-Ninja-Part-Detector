{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHScpi4V5qgM",
        "outputId": "630236b3-000f-4bd7-bd5c-b44bb878a027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess MNIST\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a CNN by subclassing keras.Model for more control\n",
        "class CNN(keras.Model):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = layers.Conv2D(32, 3, activation='relu') # Detects edges and shapes\n",
        "        self.pool1 = layers.MaxPooling2D()  # reduces spatial size, keeping important features\n",
        "        self.conv2 = layers.Conv2D(64, 3, activation='relu') # Detects edges and shapes\n",
        "        self.pool2 = layers.MaxPooling2D() # same here\n",
        "        self.flatten = layers.Flatten() #reshapes matrix into a vector\n",
        "        self.fc1 = layers.Dense(64, activation='relu')\n",
        "        self.out = layers.Dense(num_classes, activation='softmax') # turns outputs into probabilities\n",
        "\n",
        "    def call(self, x, training=False):  #defines how data flows through the network\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        return self.out(x)\n",
        "\n",
        "# Create model instance\n",
        "model = CNN(num_classes=num_classes)"
      ],
      "metadata": {
        "id": "ABtnYtVS52dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essentially, we are creating our own model class by subclassing keras.Model.\n",
        "\n"
      ],
      "metadata": {
        "id": "vgnl1SrGFUtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "\n",
        "# Measures how far off predictions are from true labels, thing network is trying to\n",
        "# minimize\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "# Create TensorFlow Datasets\n",
        "batch_size = 64\n",
        "\n",
        "# shown below, batches and shuffles data, making training faster/more stable\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)"
      ],
      "metadata": {
        "id": "4muJifcg54oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam: adaptive gradient-based optimizer that automatically adjusts the learning rate for each parameter."
      ],
      "metadata": {
        "id": "j0sxBIk6GDtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual training loop with gradient tape\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "    total_loss = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for step, (x_batch, y_batch) in enumerate(train_ds):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        acc_metric.update_state(y_batch, predictions)\n",
        "        total_loss += loss\n",
        "        steps += 1\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: loss = {loss:.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} - Loss: {total_loss/steps:.4f}, Accuracy: {acc_metric.result().numpy():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxZORFP_56B6",
        "outputId": "71771e1a-ff9b-4fd0-e142-23b4a7945c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n",
            "Step 0: loss = 2.3556\n",
            "Step 100: loss = 0.1351\n",
            "Step 200: loss = 0.3009\n",
            "Step 300: loss = 0.0883\n",
            "Step 400: loss = 0.1522\n",
            "Step 500: loss = 0.0713\n",
            "Step 600: loss = 0.0815\n",
            "Step 700: loss = 0.2047\n",
            "Step 800: loss = 0.1178\n",
            "Step 900: loss = 0.0979\n",
            "Epoch 1 - Loss: 0.1783, Accuracy: 0.9474\n",
            "\n",
            "Epoch 2/3\n",
            "Step 0: loss = 0.0239\n",
            "Step 100: loss = 0.0522\n",
            "Step 200: loss = 0.1498\n",
            "Step 300: loss = 0.0544\n",
            "Step 400: loss = 0.0409\n",
            "Step 500: loss = 0.0146\n",
            "Step 600: loss = 0.0166\n",
            "Step 700: loss = 0.0321\n",
            "Step 800: loss = 0.0679\n",
            "Step 900: loss = 0.0241\n",
            "Epoch 2 - Loss: 0.0545, Accuracy: 0.9830\n",
            "\n",
            "Epoch 3/3\n",
            "Step 0: loss = 0.0250\n",
            "Step 100: loss = 0.0131\n",
            "Step 200: loss = 0.0301\n",
            "Step 300: loss = 0.0353\n",
            "Step 400: loss = 0.0751\n",
            "Step 500: loss = 0.0596\n",
            "Step 600: loss = 0.0177\n",
            "Step 700: loss = 0.0330\n",
            "Step 800: loss = 0.0571\n",
            "Step 900: loss = 0.0185\n",
            "Epoch 3 - Loss: 0.0382, Accuracy: 0.9882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each batch does four things:\n",
        "\n",
        "Forward pass: makes predictions from current weights\n",
        "\n",
        "Loss calculation: measures how far off those precitions are\n",
        "\n",
        "Backward pass (backpropogation):uses imported function to calculate gradients\n",
        "\n",
        "Weight update: Use optimizer to adjust weights and reduce loss on the next iteration"
      ],
      "metadata": {
        "id": "t6CenwQoGdKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate manually on the test set\n",
        "test_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "for x_batch, y_batch in test_ds:\n",
        "    preds = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, preds)\n",
        "\n",
        "print(\"Test accuracy:\", test_acc_metric.result().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJpgxnVC57ys",
        "outputId": "1c46378c-f99a-4aab-c993-be057c4faaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's new: Explicitly compute gradients with tf.GradientTape().\n",
        "\n",
        "Weight updates controlled with optimizer.apply_gradients()\n",
        "\n",
        "Shows how learning happens"
      ],
      "metadata": {
        "id": "edTSlx6259UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!jupyter nbconvert --to html '/content/drive/MyDrive/HNR499/HNR499_Model2'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaL3HQ2m_HVt",
        "outputId": "299dee4d-90f8-4dcb-dd5d-cc60ea6da9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/HNR499/HNR499_Model2 to html\n",
            "[NbConvertApp] Writing 294185 bytes to /content/drive/MyDrive/HNR499/HNR499_Model.html\n"
          ]
        }
      ]
    }
  ]
}