{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wl5qG0B6pRp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# One-hot encode labels\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Glorot/Xavier initializer\n",
        "def glorot(shape):\n",
        "    fan_in = np.prod(shape[:-1])\n",
        "    fan_out = shape[-1] * np.prod(shape[:-2])\n",
        "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "    return tf.Variable(tf.random.uniform(shape, -limit, limit), trainable=True)\n",
        "\n",
        "# Convolution and dense layer weights\n",
        "W1 = glorot((3, 3, 1, 32))   # conv1: 1 input channel -> 32 filters\n",
        "b1 = tf.Variable(tf.zeros([32]), trainable=True)\n",
        "W2 = glorot((3, 3, 32, 64))  # conv2: 32 -> 64 filters\n",
        "b2 = tf.Variable(tf.zeros([64]), trainable=True)\n",
        "\n",
        "\n",
        "# Fully connected layers\n",
        "W3 = glorot((1600, 64))\n",
        "b3 = tf.Variable(tf.zeros([64]), trainable=True)\n",
        "W4 = glorot((64, num_classes))\n",
        "b4 = tf.Variable(tf.zeros([num_classes]), trainable=True)"
      ],
      "metadata": {
        "id": "ZB3tSL7U61B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of keras creating weights for us, we manually define them as trainable variables.\n",
        "\n",
        "Weight tensors initialized with Glorot, a technique to keep values in range that trains efficiently.\n",
        "\n",
        "W1 and W2: Convolutions kernels (filters)\n",
        "\n",
        "W3 and W4: Dense weight matracies\n",
        "\n",
        "b1, b2, b3, b4: Biases added after each layer\n",
        "\n",
        "Ultimately shows that neaural networks are big matricies that have operations performed on them (multiplication, addition)"
      ],
      "metadata": {
        "id": "Gx-_kaCZHH7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x): # Non linearity, allowing network to learn complex patterns\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "def maxpool2x2(x): # reduces dimensions of data, keeps strongest variation captured\n",
        "    return tf.nn.max_pool2d(x, ksize=2, strides=2, padding=\"VALID\")\n",
        "\n",
        "def flatten(x): # reshapes matracies into vectors\n",
        "    return tf.reshape(x, [tf.shape(x)[0], -1])\n",
        "\n",
        "# Forward pass\n",
        "def forward(x, training=True):\n",
        "    z1 = tf.nn.conv2d(x, W1, strides=1, padding='VALID') + b1\n",
        "    a1 = relu(z1)\n",
        "    p1 = maxpool2x2(a1)\n",
        "\n",
        "    z2 = tf.nn.conv2d(p1, W2, strides=1, padding='VALID') + b2\n",
        "    a2 = relu(z2)\n",
        "    p2 = maxpool2x2(a2)\n",
        "\n",
        "    f = flatten(p2)\n",
        "    h = relu(tf.matmul(f, W3) + b3)\n",
        "    logits = tf.matmul(h, W4) + b4\n",
        "    return tf.nn.softmax(logits), logits\n",
        "\n",
        "    # shows each step manually, from detecting edges, to matrix multiplication to\n",
        "    # proabability conversion for classification"
      ],
      "metadata": {
        "id": "yhK2Qq5B62v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YJZDtnIPHtHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare optimizer and loss\n",
        "optimizer = keras.optimizers.Adam()\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# Dataset batches\n",
        "batch_size = 64\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Training loop\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "    for xb, yb in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred, _ = forward(xb, training=True)\n",
        "            loss = loss_fn(yb, y_pred)\n",
        "        grads = tape.gradient(loss, [W1, b1, W2, b2, W3, b3, W4, b4])\n",
        "        optimizer.apply_gradients(zip(grads, [W1, b1, W2, b2, W3, b3, W4, b4]))\n",
        "        acc_metric.update_state(yb, y_pred)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} - Accuracy: {acc_metric.result().numpy():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDI3ywwU64Ny",
        "outputId": "4f56659b-8872-4065-b7bd-8e4dbeaec38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 - Accuracy: 0.9471\n",
            "Epoch 2/2 - Accuracy: 0.9836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controlling the process:\n",
        "\n",
        "Computing precitions with our forward pass, calculating loss, using it to calculate gradients, applying them to update the weight tensors."
      ],
      "metadata": {
        "id": "IqL-8VntIdPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "test_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "for xb, yb in test_ds:\n",
        "    y_pred, _ = forward(xb, training=False)\n",
        "    test_acc_metric.update_state(yb, y_pred)\n",
        "\n",
        "print(\"Test accuracy:\", test_acc_metric.result().numpy())"
      ],
      "metadata": {
        "id": "PpvWzhqF65ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3566f0d8-55fd-4144-cbcd-109e81fff985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's new?: Manually defined every weight tensor\n",
        "\n",
        "Convolution, pooling, ReLU, flattening, and dense layers are all coded with tf.nnops\n",
        "\n",
        "Nearly everything is visible math, openly followable"
      ],
      "metadata": {
        "id": "qAQ6jspF67Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!jupyter nbconvert --to html '/content/drive/MyDrive/HNR499/HNR499_Model3'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RJjV0-t_JWD",
        "outputId": "ad32a131-9a01-432b-95ae-738840570305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/HNR499/HNR499_Model3 to html\n",
            "[NbConvertApp] Writing 296094 bytes to /content/drive/MyDrive/HNR499/HNR499_Model.html\n"
          ]
        }
      ]
    }
  ]
}